name: OA Literature Mining - Weekly

on:
  schedule:
    # Runs weekly on Mondays at 2 AM EST (7 AM UTC)
    - cron: "0 7 * * 1"
  workflow_dispatch: # Allows manual triggering

# Grant write permissions to push commits
permissions:
  contents: write

jobs:
  oa-scraping:
    name: OA Literature Mining
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: "pip"
          cache-dependency-path: pubmed-literature-mining/requirements.txt

      - name: Install dependencies
        working-directory: ./pubmed-literature-mining
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create directories
        working-directory: ./pubmed-literature-mining
        run: |
          mkdir -p data/pdfs logs

      - name: Run OA PubMed scraper
        working-directory: ./pubmed-literature-mining
        env:
          UNPAYWALL_EMAIL: parker@stroomai.com
          PUBMED_EMAIL: parker@stroomai.com
          PUBMED_TOOL: PubMedLiteratureMining
          MAX_ARTICLES_PER_RUN: 100
          RELEVANCE_THRESHOLD: 70
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
        run: |
          python scripts/pubmed_scraper.py
        continue-on-error: true

      - name: Analyze and notify
        working-directory: ./pubmed-literature-mining
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPO_OWNER: ${{ github.repository_owner }}
          GITHUB_REPO_NAME: ${{ github.event.repository.name }}
          RELEVANCE_THRESHOLD: 70
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
        run: |
          python scripts/analyze_and_notify.py
        continue-on-error: true

      - name: Migrate file storage articles to Google Sheets
        working-directory: ./pubmed-literature-mining
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
        run: |
          python scripts/migrate_to_sheets.py
        continue-on-error: true

      - name: Backfill articles with missing details
        working-directory: ./pubmed-literature-mining
        env:
          UNPAYWALL_EMAIL: parker@stroomai.com
          PUBMED_EMAIL: parker@stroomai.com
          PUBMED_TOOL: PubMedLiteratureMining
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
        run: |
          python scripts/backfill_articles.py --all-missing --threshold 70
        continue-on-error: true

      - name: Generate paywalled articles list
        working-directory: ./pubmed-literature-mining
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
        run: |
          # Generate list with threshold=0 to show ALL paywalled articles
          # Use --threshold 70 if you only want high-relevance paywalled articles
          python scripts/generate_paywalled_list.py --threshold 0
        continue-on-error: true

      - name: Export sorted paywalled articles to logs
        working-directory: ./pubmed-literature-mining
        env:
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
          GOOGLE_SERVICE_ACCOUNT_EMAIL: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_EMAIL }}
          GOOGLE_PRIVATE_KEY: ${{ secrets.GOOGLE_PRIVATE_KEY }}
        run: |
          # Export detailed sorted list to logs (best to least valuable)
          # Uses file storage primarily (more reliable than Google Sheets)
          python scripts/export_paywalled_to_logs.py --threshold 0
        continue-on-error: true

      - name: Commit article data
        continue-on-error: true
        run: |
          git config user.name "OA Literature Monitor"
          git config user.email "actions@github.com"
          
          if [ -d pubmed-literature-mining/data/articles ]; then
            # Force add even if in .gitignore (we want to track article data)
            git add -f pubmed-literature-mining/data/articles/ || echo "No articles to add"
            
            if ! git diff --staged --quiet; then
              git commit -m "ðŸ“š OA Literature Update - $(date +%Y-%m-%d)" || echo "No changes to commit"
              git push || echo "Nothing to push"
            else
              echo "No new articles to commit"
            fi
          else
            echo "No articles directory found"
          fi

      - name: Generate and commit daily summary
        continue-on-error: true
        run: |
          git config user.name "OA Literature Monitor"
          git config user.email "actions@github.com"
          
          # Add summary and paywalled list
          if [ -f pubmed-literature-mining/LATEST_FINDINGS.md ]; then
            git add pubmed-literature-mining/LATEST_FINDINGS.md
          fi
          
          if [ -f pubmed-literature-mining/PAYWALLED_ARTICLES.txt ]; then
            git add pubmed-literature-mining/PAYWALLED_ARTICLES.txt
          fi
          
          if [ -f pubmed-literature-mining/PAYWALLED_ARTICLES.csv ]; then
            # CSV is in .gitignore, so force add it
            git add -f pubmed-literature-mining/PAYWALLED_ARTICLES.csv || echo "CSV file ignored, skipping"
          fi
          
          if ! git diff --staged --quiet; then
            git commit -m "ðŸ“Š OA Literature Summary - $(date +%Y-%m-%d)
            
            - Articles processed: $(cat pubmed-literature-mining/logs/daily_count.txt 2>/dev/null || echo 'N/A')
            - Paywalled articles: $(cat pubmed-literature-mining/logs/paywalled_count.txt 2>/dev/null || echo 'N/A')
            - Predictive factors: $(cat pubmed-literature-mining/logs/factors_count.txt 2>/dev/null || echo 'N/A')" || echo "Commit failed"
            git push || echo "Push failed - this is OK, data is still stored"
          else
            echo "No changes to commit"
          fi

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: oa-scraper-logs
          path: |
            pubmed-literature-mining/logs/
            pubmed-literature-mining/LATEST_FINDINGS.md
          retention-days: 7

  notify-on-failure:
    name: Notify on Failure
    runs-on: ubuntu-latest
    needs: [oa-scraping]
    if: failure()
    steps:
      - name: Check job status
        run: |
          echo "OA Literature Mining job failed"
          echo "Check the workflow logs for details"
          # Add email/Slack notification here if needed
